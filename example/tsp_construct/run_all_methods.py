import sys
import os
import numpy as np
import time
import json
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Add the parent directory to sys.path to find the modules
sys.path.append('../../')  

from llm4ad.task.optimization.tsp_construct import TSPEvaluation
from llm4ad.tools.profiler import ProfilerBase
from llm4ad.method.funsearch import FunSearch
from llm4ad.method.hillclimb import HillClimb

# Import our enhanced metrics system
from comprehensive_metrics import ComprehensiveProfiler, ExperimentComparator
from enhanced_llm_wrapper import EnhancedLLMWrapper, TokenAwareLLMFactory
from enhanced_evaluation import EnhancedTSPEvaluation

def create_simple_tsp_solver():
    """åˆ›å»ºç®€å•TSPæ±‚è§£å™¨ç”¨äºbaselineå¯¹æ¯”"""
    def simple_greedy_solver(current_node: int, destination_node: int, 
                           unvisited_nodes: np.ndarray, distance_matrix: np.ndarray) -> int:
        """
        A simple greedy TSP solver for baseline comparison.
        Always chooses the nearest unvisited node.
        
        Args:
            current_node: ID of the current node
            destination_node: ID of the destination node (not used in this simple version)
            unvisited_nodes: Array of IDs of unvisited nodes
            distance_matrix: Distance matrix of nodes
            
        Returns:
            ID of the next node to visit
        """
        if len(unvisited_nodes) == 0:
            return destination_node
            
        # Find the nearest unvisited node
        distances = distance_matrix[current_node][unvisited_nodes]
        nearest_index = np.argmin(distances)
        return unvisited_nodes[nearest_index]
    
    return simple_greedy_solver

def create_advanced_tsp_solver():
    """åˆ›å»ºæ›´é«˜çº§çš„TSPæ±‚è§£å™¨ç”¨äºå¯¹æ¯”"""
    def nearest_neighbor_with_lookahead(current_node: int, destination_node: int,
                                      unvisited_nodes: np.ndarray, distance_matrix: np.ndarray) -> int:
        """
        A more sophisticated TSP solver with 2-step lookahead.
        
        Args:
            current_node: ID of the current node
            destination_node: ID of the destination node
            unvisited_nodes: Array of IDs of unvisited nodes
            distance_matrix: Distance matrix of nodes
            
        Returns:
            ID of the next node to visit
        """
        if len(unvisited_nodes) == 0:
            return destination_node
            
        if len(unvisited_nodes) == 1:
            return unvisited_nodes[0]
            
        # Consider 2-step lookahead for better decisions
        best_next_node = None
        best_total_cost = float('inf')
        
        for next_node in unvisited_nodes:
            # Cost to go to next_node
            cost_to_next = distance_matrix[current_node][next_node]
            
            # Estimate cost from next_node to its best remaining option
            remaining_nodes = unvisited_nodes[unvisited_nodes != next_node]
            if len(remaining_nodes) > 0:
                min_cost_from_next = np.min(distance_matrix[next_node][remaining_nodes])
                total_estimated_cost = cost_to_next + min_cost_from_next
            else:
                # If this is the last node, add cost to return to destination
                total_estimated_cost = cost_to_next + distance_matrix[next_node][destination_node]
            
            if total_estimated_cost < best_total_cost:
                best_total_cost = total_estimated_cost
                best_next_node = next_node
                
        return best_next_node if best_next_node is not None else unvisited_nodes[0]
    
    return nearest_neighbor_with_lookahead


# Check for required environment variables
def check_api_key():
    """Check if API key is available in environment variables."""
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        raise ValueError("DEEPSEEK_API_KEY not found in environment variables. Please create a .env file with your API key.")
    return api_key

def run_comprehensive_experiment(experiment_name: str, llm_config: dict, 
                                method_config: dict, method_type: str = 'funsearch', num_runs: int = 3):
    """
    è¿è¡Œç»¼åˆå®éªŒï¼Œæ”¶é›†å¤šç»´åº¦æŒ‡æ ‡
    
    Args:
        experiment_name: å®éªŒåç§°
        llm_config: LLMé…ç½®
        method_config: æ–¹æ³•é…ç½®
        method_type: æ–¹æ³•ç±»å‹ ('funsearch', 'hillclimb')
        num_runs: è¿è¡Œæ¬¡æ•°
    
    Returns:
        å®éªŒç»“æœå’ŒæŒ‡æ ‡
    """
    print(f"\n{'='*60}")
    print(f"è¿è¡ŒTSPå®éªŒ: {experiment_name} (æ–¹æ³•: {method_type.upper()})")
    print(f"{'='*60}")
    
    # åˆ›å»ºå®éªŒå¯¹æ¯”å™¨
    comparator = ExperimentComparator()
    
    all_results = []
    
    for run_id in range(num_runs):
        print(f"\n--- è¿è¡Œ {run_id + 1}/{num_runs} ---")
        
        # åˆ›å»ºç»¼åˆprofiler
        profiler = ComprehensiveProfiler(
            log_dir=f"comprehensive_logs/{experiment_name}/run_{run_id}",
            track_tokens=True
        )
        
        # åˆ›å»ºå¢å¼ºç‰ˆLLM
        enhanced_llm = TokenAwareLLMFactory.create_enhanced_llm(
            host=llm_config['host'],
            key=llm_config['key'],
            model=llm_config['model'],
            profiler=profiler,
            **llm_config.get('kwargs', {})
        )
        
        # åˆ›å»ºå¢å¼ºç‰ˆè¯„ä¼°å™¨
        enhanced_task = EnhancedTSPEvaluation(
            profiler=profiler,
            stability_runs=3,  # æ¯ä¸ªè§£è¿è¡Œ3æ¬¡ä»¥æµ‹è¯•ç¨³å®šæ€§
            collect_detailed_metrics=True
        )
        
        # å…ˆæµ‹è¯•baselineæ±‚è§£å™¨
        print("æµ‹è¯•ç®€å•è´ªå¿ƒbaseline...")
        simple_solver = create_simple_tsp_solver()
        baseline_result = enhanced_task.evaluate_with_stability_analysis(
            simple_solver, 
            function_id="simple_greedy_baseline"
        )
        print(f"ç®€å•è´ªå¿ƒæœ€ä½³åˆ†æ•°: {baseline_result['summary']['best_score']:.3f}")
        print(f"ç®€å•è´ªå¿ƒç¨³å®šæ€§: {baseline_result['stability_metrics']['stability_score']:.3f}")
        
        # æµ‹è¯•é«˜çº§baselineæ±‚è§£å™¨
        print("æµ‹è¯•é«˜çº§baseline...")
        advanced_solver = create_advanced_tsp_solver()
        advanced_baseline_result = enhanced_task.evaluate_with_stability_analysis(
            advanced_solver,
            function_id="advanced_baseline"
        )
        print(f"é«˜çº§baselineæœ€ä½³åˆ†æ•°: {advanced_baseline_result['summary']['best_score']:.3f}")
        print(f"é«˜çº§baselineç¨³å®šæ€§: {advanced_baseline_result['stability_metrics']['stability_score']:.3f}")
        
        # æ ¹æ®æ–¹æ³•ç±»å‹åˆ›å»ºä¸åŒçš„æ–¹æ³•å®ä¾‹
        if method_type.lower() == 'funsearch':
            method = FunSearch(
                llm=enhanced_llm,
                profiler=profiler,
                evaluation=enhanced_task,
                **method_config
            )
        elif method_type.lower() == 'hillclimb':
            method = HillClimb(
                llm=enhanced_llm,
                profiler=profiler,
                evaluation=enhanced_task,
                **method_config
            )
        else:
            raise ValueError(f"Unsupported method type: {method_type}. Supported types: 'funsearch', 'hillclimb'")
        
        # è¿è¡Œä¼˜åŒ–
        print(f"è¿è¡Œ{method_type.upper()}ä¼˜åŒ–...")
        start_time = time.time()
        method.run()
        total_time = time.time() - start_time
        
        # æ”¶é›†è¿è¡Œåçš„æŒ‡æ ‡
        profiler.calculate_final_metrics()
        llm_usage = enhanced_llm.get_usage_summary()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        evaluation_report = enhanced_task.export_comprehensive_report(
            f"comprehensive_logs/{experiment_name}/run_{run_id}/evaluation_report.json"
        )
        
        # ä¿å­˜LLMä½¿ç”¨æ—¥å¿—
        enhanced_llm.export_detailed_log(
            f"comprehensive_logs/{experiment_name}/run_{run_id}/llm_usage.json"
        )
        
        # ä¿å­˜ç»¼åˆæŒ‡æ ‡
        profiler.save_metrics(
            f"comprehensive_logs/{experiment_name}/run_{run_id}/comprehensive_metrics.json"
        )
        
        # æ”¶é›†æœ¬æ¬¡è¿è¡Œçš„ç»“æœ
        run_result = {
            'run_id': run_id,
            'experiment_name': experiment_name,
            'method_type': method_type,
            'total_time': total_time,
            'baseline_performance': {
                'simple_greedy': baseline_result,
                'advanced_baseline': advanced_baseline_result
            },
            'final_metrics': {
                'best_score': profiler.metrics.best_score,
                'mean_score': profiler.metrics.mean_score,
                'std_score': profiler.metrics.std_score,
                'total_evaluator_calls': profiler.metrics.total_evaluator_calls,
                'successful_evaluations': profiler.metrics.successful_evaluations,
                'convergence_generation': profiler.metrics.convergence_generation
            },
            'llm_usage': llm_usage,
            'stability_metrics': profiler.get_stability_metrics(),
            'efficiency_metrics': profiler.get_efficiency_metrics(),
            'evaluation_diversity': enhanced_task.analyze_solution_diversity(),
            'trend_analysis': enhanced_task.generate_performance_trend_analysis()
        }
        
        all_results.append(run_result)
        
        # æ·»åŠ åˆ°å®éªŒå¯¹æ¯”å™¨
        comparator.add_experiment(f"{experiment_name}_run_{run_id}", profiler)
        
        print(f"å®Œæˆè¿è¡Œ {run_id + 1}:")
        print(f"  æœ€ä½³åˆ†æ•°: {profiler.metrics.best_score:.3f}")
        print(f"  è¯„ä¼°æ¬¡æ•°: {profiler.metrics.total_evaluator_calls}")
        print(f"  æ€»Tokenæ•°: {llm_usage['total_tokens']}")
        print(f"  æ€»æˆæœ¬: ${llm_usage['cost_analysis']['total_cost_usd']:.6f}")
        print(f"  æˆåŠŸç‡: {profiler.get_stability_metrics().get('success_rate', 0):.3f}")
    
    # ç”Ÿæˆå®éªŒé—´å¯¹æ¯”æŠ¥å‘Š
    comparison_report = comparator.generate_summary_report(
        f"comprehensive_logs/{experiment_name}/experiment_comparison.json"
    )
    
    # è®¡ç®—ç»Ÿè®¡æ‘˜è¦
    experiment_summary = calculate_experiment_statistics(all_results, experiment_name)
    
    # ä¿å­˜å®Œæ•´çš„å®éªŒç»“æœ
    complete_results = {
        'experiment_name': experiment_name,
        'method_type': method_type,
        'num_runs': num_runs,
        'experiment_summary': experiment_summary,
        'individual_runs': all_results,
        'comparison_report': comparison_report,
        'configurations': {
            'llm_config': llm_config,
            'method_config': method_config
        }
    }
    
    with open(f"comprehensive_logs/{experiment_name}/complete_results.json", 'w') as f:
        json.dump(complete_results, f, indent=2)
    
    print(f"\nå®éªŒ '{experiment_name}' å®Œæˆ!")
    print_experiment_summary(experiment_summary)
    
    return complete_results

def calculate_experiment_statistics(results: list, experiment_name: str) -> dict:
    """è®¡ç®—å®éªŒç»Ÿè®¡ä¿¡æ¯"""
    # æå–å…³é”®æŒ‡æ ‡
    best_scores = [r['final_metrics']['best_score'] for r in results]
    mean_scores = [r['final_metrics']['mean_score'] for r in results]
    evaluator_calls = [r['final_metrics']['total_evaluator_calls'] for r in results]
    total_costs = [r['llm_usage']['cost_analysis']['total_cost_usd'] for r in results]
    total_tokens = [r['llm_usage']['total_tokens'] for r in results]
    success_rates = [r['stability_metrics'].get('success_rate', 0) for r in results]
    
    # æå–baselineæ€§èƒ½ - ä½¿ç”¨ç®€å•è´ªå¿ƒä½œä¸ºä¸»è¦åŸºå‡†
    baseline_scores = [r['baseline_performance']['simple_greedy']['summary']['best_score'] for r in results]
    baseline_stability = [r['baseline_performance']['simple_greedy']['stability_metrics']['stability_score'] for r in results]
    
    # æå–é«˜çº§baselineæ€§èƒ½
    advanced_baseline_scores = [r['baseline_performance']['advanced_baseline']['summary']['best_score'] for r in results]
    
    # è®¡ç®—æ”¹è¿›æ¯”ä¾‹
    improvements_simple = [(best - baseline) / abs(baseline) * 100 
                          for best, baseline in zip(best_scores, baseline_scores) 
                          if baseline != 0]
    
    improvements_advanced = [(best - baseline) / abs(baseline) * 100 
                           for best, baseline in zip(best_scores, advanced_baseline_scores) 
                           if baseline != 0]
    
    return {
        'performance_metrics': {
            'best_score': {
                'mean': np.mean(best_scores),
                'std': np.std(best_scores),
                'min': min(best_scores),
                'max': max(best_scores),
                'median': np.median(best_scores)
            },
            'mean_score': {
                'mean': np.mean(mean_scores),
                'std': np.std(mean_scores),
                'median': np.median(mean_scores)
            },
            'improvement_over_simple_baseline': {
                'mean': np.mean(improvements_simple) if improvements_simple else 0,
                'std': np.std(improvements_simple) if improvements_simple else 0,
                'min': min(improvements_simple) if improvements_simple else 0,
                'max': max(improvements_simple) if improvements_simple else 0
            },
            'improvement_over_advanced_baseline': {
                'mean': np.mean(improvements_advanced) if improvements_advanced else 0,
                'std': np.std(improvements_advanced) if improvements_advanced else 0,
                'min': min(improvements_advanced) if improvements_advanced else 0,
                'max': max(improvements_advanced) if improvements_advanced else 0
            }
        },
        'cost_metrics': {
            'total_cost_usd': {
                'mean': np.mean(total_costs),
                'std': np.std(total_costs),
                'min': min(total_costs),
                'max': max(total_costs),
                'total': sum(total_costs)
            },
            'total_tokens': {
                'mean': np.mean(total_tokens),
                'std': np.std(total_tokens),
                'min': min(total_tokens),
                'max': max(total_tokens),
                'total': sum(total_tokens)
            }
        },
        'efficiency_metrics': {
            'evaluator_calls': {
                'mean': np.mean(evaluator_calls),
                'std': np.std(evaluator_calls),
                'total': sum(evaluator_calls)
            },
            'cost_efficiency': {
                'score_per_dollar': [score / cost for score, cost in zip(best_scores, total_costs) if cost > 0],
                'score_per_evaluation': [score / calls for score, calls in zip(best_scores, evaluator_calls) if calls > 0]
            }
        },
        'stability_metrics': {
            'success_rate': {
                'mean': np.mean(success_rates),
                'std': np.std(success_rates),
                'min': min(success_rates),
                'max': max(success_rates)
            },
            'baseline_comparison': {
                'simple_baseline_score_mean': np.mean(baseline_scores),
                'simple_baseline_stability_mean': np.mean(baseline_stability),
                'advanced_baseline_score_mean': np.mean(advanced_baseline_scores),
                'improvement_consistency_simple': np.std(improvements_simple) if improvements_simple else float('inf'),
                'improvement_consistency_advanced': np.std(improvements_advanced) if improvements_advanced else float('inf')
            }
        }
    }

def print_experiment_summary(summary: dict):
    """æ‰“å°å®éªŒæ‘˜è¦"""
    print("\n" + "="*80)
    print("TSPå®éªŒç»Ÿè®¡æ‘˜è¦")
    print("="*80)
    
    perf = summary['performance_metrics']
    cost = summary['cost_metrics']
    eff = summary['efficiency_metrics']
    stab = summary['stability_metrics']
    
    print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"  æœ€ä½³åˆ†æ•°: {perf['best_score']['mean']:.3f} Â± {perf['best_score']['std']:.3f}")
    print(f"  åˆ†æ•°èŒƒå›´: [{perf['best_score']['min']:.3f}, {perf['best_score']['max']:.3f}]")
    print(f"  ç›¸å¯¹ç®€å•baselineæ”¹è¿›: {perf['improvement_over_simple_baseline']['mean']:.1f}% Â± {perf['improvement_over_simple_baseline']['std']:.1f}%")
    print(f"  ç›¸å¯¹é«˜çº§baselineæ”¹è¿›: {perf['improvement_over_advanced_baseline']['mean']:.1f}% Â± {perf['improvement_over_advanced_baseline']['std']:.1f}%")
    
    print(f"\nğŸ’° æˆæœ¬æŒ‡æ ‡:")
    print(f"  å¹³å‡æ€»æˆæœ¬: ${cost['total_cost_usd']['mean']:.6f} Â± ${cost['total_cost_usd']['std']:.6f}")
    print(f"  å¹³å‡Tokenä½¿ç”¨: {cost['total_tokens']['mean']:.0f} Â± {cost['total_tokens']['std']:.0f}")
    print(f"  æ€»æˆæœ¬: ${cost['total_cost_usd']['total']:.6f}")
    
    print(f"\nâš¡ æ•ˆç‡æŒ‡æ ‡:")
    print(f"  å¹³å‡è¯„ä¼°æ¬¡æ•°: {eff['evaluator_calls']['mean']:.0f} Â± {eff['evaluator_calls']['std']:.0f}")
    if eff['cost_efficiency']['score_per_dollar']:
        print(f"  æˆæœ¬æ•ˆç‡ (åˆ†æ•°/$): {np.mean(eff['cost_efficiency']['score_per_dollar']):.0f}")
    if eff['cost_efficiency']['score_per_evaluation']:
        print(f"  è¯„ä¼°æ•ˆç‡ (åˆ†æ•°/è¯„ä¼°): {np.mean(eff['cost_efficiency']['score_per_evaluation']):.3f}")
    
    print(f"\nğŸ¯ ç¨³å®šæ€§æŒ‡æ ‡:")
    print(f"  æˆåŠŸç‡: {stab['success_rate']['mean']:.3f} Â± {stab['success_rate']['std']:.3f}")
    print(f"  ç®€å•Baselineåˆ†æ•°: {stab['baseline_comparison']['simple_baseline_score_mean']:.3f}")
    print(f"  é«˜çº§Baselineåˆ†æ•°: {stab['baseline_comparison']['advanced_baseline_score_mean']:.3f}")
    print(f"  æ”¹è¿›ä¸€è‡´æ€§(ç®€å•): {stab['baseline_comparison']['improvement_consistency_simple']:.3f}")
    print(f"  æ”¹è¿›ä¸€è‡´æ€§(é«˜çº§): {stab['baseline_comparison']['improvement_consistency_advanced']:.3f}")

def run_comparative_experiments():
    """è¿è¡Œå¯¹æ¯”å®éªŒ"""
    
    # åŸºç¡€LLMé…ç½®
    base_llm_config = {
        'host': 'api.deepseek.com',
        'key': os.getenv('DEEPSEEK_API_KEY'),
        'model': 'deepseek-chat',
        'kwargs': {'timeout': 300}
    }
    
    # å®éªŒé…ç½® - åŒ…æ‹¬FunSearchå’ŒHillClimbå¯¹æ¯”
    experiments = [
        # FunSearchå®éªŒé…ç½®
        {
            'name': 'tsp_funsearch_standard',
            'method_type': 'funsearch',
            'llm_config': base_llm_config,
            'method_config': {
                'max_sample_nums': 20,
                'num_samplers': 1,
                'num_evaluators': 1,
                'samples_per_prompt': 4,
                'debug_mode': False
            }
        },
        # HillClimbå®éªŒé…ç½®
        {
            'name': 'tsp_hillclimb_standard',
            'method_type': 'hillclimb',
            'llm_config': base_llm_config,
            'method_config': {
                'max_sample_nums': 20,
                'num_samplers': 1,
                'num_evaluators': 1,
                'debug_mode': False
            }
        }
    ]
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    all_experiment_results = {}
    
    for exp_config in experiments:
        results = run_comprehensive_experiment(
            experiment_name=exp_config['name'],
            llm_config=exp_config['llm_config'],
            method_config=exp_config['method_config'],
            method_type=exp_config['method_type'],
            num_runs=3  # è¿è¡Œ3æ¬¡è·å¾—æ›´å¥½çš„ç»Ÿè®¡
        )
        all_experiment_results[exp_config['name']] = results
    
    # ç”Ÿæˆè·¨å®éªŒå¯¹æ¯”æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("è·¨å®éªŒå¯¹æ¯”åˆ†æ (FunSearch vs HillClimb)")
    print(f"{'='*80}")
    
    comparison_table = []
    for exp_name, results in all_experiment_results.items():
        summary = results['experiment_summary']
        comparison_table.append({
            'experiment': exp_name,
            'method_type': results['method_type'],
            'avg_best_score': summary['performance_metrics']['best_score']['mean'],
            'score_std': summary['performance_metrics']['best_score']['std'],
            'avg_cost': summary['cost_metrics']['total_cost_usd']['mean'],
            'avg_tokens': summary['cost_metrics']['total_tokens']['mean'],
            'avg_evaluations': summary['efficiency_metrics']['evaluator_calls']['mean'],
            'success_rate': summary['stability_metrics']['success_rate']['mean'],
            'improvement_simple': summary['performance_metrics']['improvement_over_simple_baseline']['mean'],
            'improvement_advanced': summary['performance_metrics']['improvement_over_advanced_baseline']['mean']
        })
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print(f"\n{'å®éªŒåç§°':<25} {'æ–¹æ³•':<10} {'å¹³å‡æœ€ä½³åˆ†æ•°':<12} {'åˆ†æ•°æ ‡å‡†å·®':<10} {'å¹³å‡æˆæœ¬($)':<12} {'å¹³å‡Token':<10} {'è¯„ä¼°æ¬¡æ•°':<8} {'æˆåŠŸç‡':<8} {'æ”¹è¿›ç®€å•(%)':<12} {'æ”¹è¿›é«˜çº§(%)':<12}")
    print("-" * 140)
    for row in comparison_table:
        print(f"{row['experiment']:<25} {row['method_type']:<10} {row['avg_best_score']:<12.3f} {row['score_std']:<10.3f} {row['avg_cost']:<12.6f} {row['avg_tokens']:<10.0f} {row['avg_evaluations']:<8.0f} {row['success_rate']:<8.3f} {row['improvement_simple']:<12.1f} {row['improvement_advanced']:<12.1f}")
    
    # æŒ‰æ–¹æ³•ç±»å‹åˆ†ç»„åˆ†æ
    funsearch_results = [row for row in comparison_table if row['method_type'] == 'funsearch']
    hillclimb_results = [row for row in comparison_table if row['method_type'] == 'hillclimb']
    
    print(f"\nğŸ“Š æ–¹æ³•å¯¹æ¯”åˆ†æ:")
    if funsearch_results:
        fs_avg_score = np.mean([row['avg_best_score'] for row in funsearch_results])
        fs_avg_cost = np.mean([row['avg_cost'] for row in funsearch_results])
        fs_avg_success = np.mean([row['success_rate'] for row in funsearch_results])
        fs_avg_improvement_simple = np.mean([row['improvement_simple'] for row in funsearch_results])
        fs_avg_improvement_advanced = np.mean([row['improvement_advanced'] for row in funsearch_results])
        
        print(f"  FUNSEARCHå¹³å‡è¡¨ç°:")
        print(f"    å¹³å‡æœ€ä½³åˆ†æ•°: {fs_avg_score:.3f}")
        print(f"    å¹³å‡æˆæœ¬: ${fs_avg_cost:.6f}")
        print(f"    å¹³å‡æˆåŠŸç‡: {fs_avg_success:.3f}")
        print(f"    å¹³å‡æ”¹è¿›(ç®€å•baseline): {fs_avg_improvement_simple:.1f}%")
        print(f"    å¹³å‡æ”¹è¿›(é«˜çº§baseline): {fs_avg_improvement_advanced:.1f}%")
    
    if hillclimb_results:
        hc_avg_score = np.mean([row['avg_best_score'] for row in hillclimb_results])
        hc_avg_cost = np.mean([row['avg_cost'] for row in hillclimb_results])
        hc_avg_success = np.mean([row['success_rate'] for row in hillclimb_results])
        hc_avg_improvement_simple = np.mean([row['improvement_simple'] for row in hillclimb_results])
        hc_avg_improvement_advanced = np.mean([row['improvement_advanced'] for row in hillclimb_results])
        
        print(f"  HILLCLIMBå¹³å‡è¡¨ç°:")
        print(f"    å¹³å‡æœ€ä½³åˆ†æ•°: {hc_avg_score:.3f}")
        print(f"    å¹³å‡æˆæœ¬: ${hc_avg_cost:.6f}")
        print(f"    å¹³å‡æˆåŠŸç‡: {hc_avg_success:.3f}")
        print(f"    å¹³å‡æ”¹è¿›(ç®€å•baseline): {hc_avg_improvement_simple:.1f}%")
        print(f"    å¹³å‡æ”¹è¿›(é«˜çº§baseline): {hc_avg_improvement_advanced:.1f}%")
    
    if funsearch_results and hillclimb_results:
        print(f"  æ–¹æ³•å¯¹æ¯”:")
        print(f"    åˆ†æ•°å·®å¼‚: FunSearch vs HillClimb = {fs_avg_score:.3f} vs {hc_avg_score:.3f} ({((fs_avg_score - hc_avg_score) / hc_avg_score * 100):+.1f}%)")
        print(f"    æˆæœ¬æ•ˆç‡: FunSearch={fs_avg_score/fs_avg_cost:.0f} vs HillClimb={hc_avg_score/hc_avg_cost:.0f} (åˆ†æ•°/ç¾å…ƒ)")
        print(f"    ç¨³å®šæ€§: FunSearch={fs_avg_success:.3f} vs HillClimb={hc_avg_success:.3f}")
    
    # ä¿å­˜è·¨å®éªŒå¯¹æ¯”ç»“æœ
    cross_experiment_analysis = {
        'timestamp': time.time(),
        'experiments': list(all_experiment_results.keys()),
        'comparison_table': comparison_table,
        'method_comparison': {
            'funsearch_summary': {
                'avg_score': fs_avg_score if funsearch_results else None,
                'avg_cost': fs_avg_cost if funsearch_results else None,
                'avg_success_rate': fs_avg_success if funsearch_results else None,
                'avg_improvement_simple': fs_avg_improvement_simple if funsearch_results else None,
                'avg_improvement_advanced': fs_avg_improvement_advanced if funsearch_results else None,
                'configurations': [row['experiment'] for row in funsearch_results]
            },
            'hillclimb_summary': {
                'avg_score': hc_avg_score if hillclimb_results else None,
                'avg_cost': hc_avg_cost if hillclimb_results else None,
                'avg_success_rate': hc_avg_success if hillclimb_results else None,
                'avg_improvement_simple': hc_avg_improvement_simple if hillclimb_results else None,
                'avg_improvement_advanced': hc_avg_improvement_advanced if hillclimb_results else None,
                'configurations': [row['experiment'] for row in hillclimb_results]
            } if hillclimb_results else None
        },
        'detailed_results': all_experiment_results
    }
    
    os.makedirs('comprehensive_logs/cross_experiment_analysis', exist_ok=True)
    with open('comprehensive_logs/cross_experiment_analysis/tsp_funsearch_vs_hillclimb_comparison.json', 'w') as f:
        json.dump(cross_experiment_analysis, f, indent=2)
    
    # æ‰¾å‡ºæœ€ä¼˜é…ç½®
    best_by_score = max(comparison_table, key=lambda x: x['avg_best_score'])
    best_by_efficiency = max(comparison_table, key=lambda x: x['avg_best_score'] / max(x['avg_cost'], 1e-8))
    most_stable = max(comparison_table, key=lambda x: x['success_rate'])
    best_improvement_simple = max(comparison_table, key=lambda x: x['improvement_simple'])
    best_improvement_advanced = max(comparison_table, key=lambda x: x['improvement_advanced'])
    
    print(f"\nğŸ† æœ€ä¼˜é…ç½®åˆ†æ:")
    print(f"  æœ€é«˜åˆ†æ•°: {best_by_score['experiment']} ({best_by_score['method_type']}) - åˆ†æ•°: {best_by_score['avg_best_score']:.3f}")
    print(f"  æœ€é«˜æ•ˆç‡: {best_by_efficiency['experiment']} ({best_by_efficiency['method_type']}) - åˆ†æ•°/æˆæœ¬: {best_by_efficiency['avg_best_score']/max(best_by_efficiency['avg_cost'], 1e-8):.0f}")
    print(f"  æœ€ç¨³å®š: {most_stable['experiment']} ({most_stable['method_type']}) - æˆåŠŸç‡: {most_stable['success_rate']:.3f}")
    print(f"  æœ€å¤§æ”¹è¿›(ç®€å•): {best_improvement_simple['experiment']} ({best_improvement_simple['method_type']}) - æ”¹è¿›: {best_improvement_simple['improvement_simple']:.1f}%")
    print(f"  æœ€å¤§æ”¹è¿›(é«˜çº§): {best_improvement_advanced['experiment']} ({best_improvement_advanced['method_type']}) - æ”¹è¿›: {best_improvement_advanced['improvement_advanced']:.1f}%")
    
    return cross_experiment_analysis

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹TSPç»¼åˆå¤šç»´åº¦è¯„ä»·æŒ‡æ ‡å®éªŒ (FunSearch vs HillClimb)...")
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs('comprehensive_logs', exist_ok=True)
    
    # è¿è¡Œå¯¹æ¯”å®éªŒ
    cross_experiment_results = run_comparative_experiments()
    
    print(f"\nâœ… æ‰€æœ‰TSPå®éªŒå®Œæˆ!")
    print(f"ğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: comprehensive_logs/")
    print(f"ğŸ“Š è·¨å®éªŒå¯¹æ¯”æŠ¥å‘Š: comprehensive_logs/cross_experiment_analysis/tsp_funsearch_vs_hillclimb_comparison.json")

if __name__ == '__main__':
    main() 