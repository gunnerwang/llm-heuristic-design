import sys
import os
import numpy as np
import time
import json
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Add the parent directory to sys.path to find the modules
sys.path.append('../../')  

from llm4ad.task.optimization.agv_drone_scheduling.evaluation import VehicleSchedulingEvaluation
from llm4ad.tools.profiler import ProfilerBase
from llm4ad.method.funsearch import FunSearch
from llm4ad.method.hillclimb import HillClimb

# Import our enhanced metrics system
from comprehensive_metrics import ComprehensiveProfiler, ExperimentComparator
from enhanced_llm_wrapper import EnhancedLLMWrapper, TokenAwareLLMFactory
from enhanced_evaluation import EnhancedVehicleSchedulingEvaluation

def create_simple_scheduler():
    """åˆ›å»ºç®€å•è°ƒåº¦å™¨ç”¨äºbaselineå¯¹æ¯”"""
    def simple_scheduler(env, vehicle_index, current_node):
        """
        A simple scheduler for testing the AGV and drone environment.
        """
        # First check if vehicle needs charging
        if env.check_action4_valide(vehicle_index):
            return 4
            
        # Determine vehicle type
        is_drone = (env.vehicle_info[vehicle_index, 4] == 1)
        
        # Apply different strategies based on vehicle type
        if is_drone:
            # Drones are faster, prioritize longer routes
            if current_node == 'A':
                if env.check_action0_valide(vehicle_index):
                    return 0  # Take part from A to B
                elif env.check_action2_valide(vehicle_index):
                    return 2  # Take part from A to C
            elif current_node == 'B':
                if env.check_action1_valide(vehicle_index):
                    return 1  # Recycle tray from B to A
            elif current_node == 'C':
                if env.check_action3_valide(vehicle_index):
                    return 3  # Carry part from C to B
        else:
            # AGVs - standard priority
            if current_node == 'A':
                # First try to take parts directly to B
                if env.check_action0_valide(vehicle_index):
                    return 0
                # If B is full or no parts for B, consider C
                elif env.check_action2_valide(vehicle_index):
                    return 2
            elif current_node == 'B':
                if env.check_action1_valide(vehicle_index):
                    return 1  # Recycle tray
            elif current_node == 'C':
                if env.check_action3_valide(vehicle_index):
                    return 3  # Take from C to B
        
        # No valid action
        return -1
    
    return simple_scheduler

def run_comprehensive_experiment(experiment_name: str, llm_config: dict, 
                                method_config: dict, method_type: str = 'funsearch', num_runs: int = 3):
    """
    è¿è¡Œç»¼åˆå®éªŒï¼Œæ”¶é›†å¤šç»´åº¦æŒ‡æ ‡
    
    Args:
        experiment_name: å®éªŒåç§°
        llm_config: LLMé…ç½®
        method_config: æ–¹æ³•é…ç½®
        method_type: æ–¹æ³•ç±»å‹ ('funsearch' æˆ– 'hillclimb')
        num_runs: è¿è¡Œæ¬¡æ•°
    
    Returns:
        å®éªŒç»“æœå’ŒæŒ‡æ ‡
    """
    print(f"\n{'='*60}")
    print(f"è¿è¡Œå®éªŒ: {experiment_name} (æ–¹æ³•: {method_type.upper()})")
    print(f"{'='*60}")
    
    # åˆ›å»ºå®éªŒå¯¹æ¯”å™¨
    comparator = ExperimentComparator()
    
    all_results = []
    
    for run_id in range(num_runs):
        print(f"\n--- è¿è¡Œ {run_id + 1}/{num_runs} ---")
        
        # åˆ›å»ºç»¼åˆprofiler
        profiler = ComprehensiveProfiler(
            log_dir=f"comprehensive_logs/{experiment_name}/run_{run_id}",
            track_tokens=True
        )
        
        # åˆ›å»ºå¢å¼ºç‰ˆLLM
        enhanced_llm = TokenAwareLLMFactory.create_enhanced_llm(
            host=llm_config['host'],
            key=llm_config['key'],
            model=llm_config['model'],
            profiler=profiler,
            **llm_config.get('kwargs', {})
        )
        
        
        # åˆ›å»ºå¢å¼ºç‰ˆè¯„ä¼°å™¨
        enhanced_task = EnhancedVehicleSchedulingEvaluation(
            profiler=profiler,
            stability_runs=3,  # æ¯ä¸ªè§£è¿è¡Œ3æ¬¡ä»¥æµ‹è¯•ç¨³å®šæ€§
            collect_detailed_metrics=True
        )
        
        # å…ˆæµ‹è¯•baseline
        print("æµ‹è¯•baselineè°ƒåº¦å™¨...")
        baseline_scheduler = create_simple_scheduler()
        baseline_result = enhanced_task.evaluate_with_stability_analysis(
            baseline_scheduler, 
            function_id="baseline"
        )
        print(f"Baselineæœ€ä½³åˆ†æ•°: {baseline_result['summary']['best_score']:.3f}")
        print(f"Baselineç¨³å®šæ€§: {baseline_result['stability_metrics']['stability_score']:.3f}")
        
        # æ ¹æ®æ–¹æ³•ç±»å‹åˆ›å»ºä¸åŒçš„æ–¹æ³•å®ä¾‹
        if method_type.lower() == 'funsearch':
            method = FunSearch(
                llm=enhanced_llm,
                profiler=profiler,
                evaluation=enhanced_task,
                **method_config
            )
        elif method_type.lower() == 'hillclimb':
            method = HillClimb(
                llm=enhanced_llm,
                profiler=profiler,
                evaluation=enhanced_task,
                **method_config
            )
        else:
            raise ValueError(f"Unsupported method type: {method_type}. Supported types: 'funsearch', 'hillclimb'")
        
        # è¿è¡Œä¼˜åŒ–
        print(f"è¿è¡Œ{method_type.upper()}ä¼˜åŒ–...")
        start_time = time.time()
        method.run()
        total_time = time.time() - start_time
        
        # æ”¶é›†è¿è¡Œåçš„æŒ‡æ ‡
        profiler.calculate_final_metrics()
        llm_usage = enhanced_llm.get_usage_summary()
        
        # è·å–æœ€ç»ˆçš„æœ€ä½³è§£
        best_function = profiler.metrics.best_score
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        evaluation_report = enhanced_task.export_comprehensive_report(
            f"comprehensive_logs/{experiment_name}/run_{run_id}/evaluation_report.json"
        )
        
        # ä¿å­˜LLMä½¿ç”¨æ—¥å¿—
        enhanced_llm.export_detailed_log(
            f"comprehensive_logs/{experiment_name}/run_{run_id}/llm_usage.json"
        )
        
        # ä¿å­˜ç»¼åˆæŒ‡æ ‡
        profiler.save_metrics(
            f"comprehensive_logs/{experiment_name}/run_{run_id}/comprehensive_metrics.json"
        )
        
        # æ”¶é›†æœ¬æ¬¡è¿è¡Œçš„ç»“æœ
        run_result = {
            'run_id': run_id,
            'experiment_name': experiment_name,
            'method_type': method_type,
            'total_time': total_time,
            'baseline_performance': baseline_result,
            'final_metrics': {
                'best_score': profiler.metrics.best_score,
                'mean_score': profiler.metrics.mean_score,
                'std_score': profiler.metrics.std_score,
                'total_evaluator_calls': profiler.metrics.total_evaluator_calls,
                'successful_evaluations': profiler.metrics.successful_evaluations,
                'convergence_generation': profiler.metrics.convergence_generation
            },
            'llm_usage': llm_usage,
            'stability_metrics': profiler.get_stability_metrics(),
            'efficiency_metrics': profiler.get_efficiency_metrics(),
            'evaluation_diversity': enhanced_task.analyze_solution_diversity(),
            'trend_analysis': enhanced_task.generate_performance_trend_analysis()
        }
        
        all_results.append(run_result)
        
        # æ·»åŠ åˆ°å®éªŒå¯¹æ¯”å™¨
        comparator.add_experiment(f"{experiment_name}_run_{run_id}", profiler)
        
        print(f"å®Œæˆè¿è¡Œ {run_id + 1}:")
        print(f"  æœ€ä½³åˆ†æ•°: {profiler.metrics.best_score:.3f}")
        print(f"  è¯„ä¼°æ¬¡æ•°: {profiler.metrics.total_evaluator_calls}")
        print(f"  æ€»Tokenæ•°: {llm_usage['total_tokens']}")
        print(f"  æ€»æˆæœ¬: ${llm_usage['cost_analysis']['total_cost_usd']:.6f}")
        print(f"  æˆåŠŸç‡: {profiler.get_stability_metrics().get('success_rate', 0):.3f}")
    
    # ç”Ÿæˆå®éªŒé—´å¯¹æ¯”æŠ¥å‘Š
    comparison_report = comparator.generate_summary_report(
        f"comprehensive_logs/{experiment_name}/experiment_comparison.json"
    )
    
    # è®¡ç®—ç»Ÿè®¡æ‘˜è¦
    experiment_summary = calculate_experiment_statistics(all_results, experiment_name)
    
    # ä¿å­˜å®Œæ•´çš„å®éªŒç»“æœ
    complete_results = {
        'experiment_name': experiment_name,
        'method_type': method_type,
        'num_runs': num_runs,
        'experiment_summary': experiment_summary,
        'individual_runs': all_results,
        'comparison_report': comparison_report,
        'configurations': {
            'llm_config': llm_config,
            'method_config': method_config
        }
    }
    
    with open(f"comprehensive_logs/{experiment_name}/complete_results.json", 'w') as f:
        json.dump(complete_results, f, indent=2)
    
    print(f"\nå®éªŒ '{experiment_name}' å®Œæˆ!")
    print_experiment_summary(experiment_summary)
    
    return complete_results

def calculate_experiment_statistics(results: list, experiment_name: str) -> dict:
    """è®¡ç®—å®éªŒç»Ÿè®¡ä¿¡æ¯"""
    # æå–å…³é”®æŒ‡æ ‡
    best_scores = [r['final_metrics']['best_score'] for r in results]
    mean_scores = [r['final_metrics']['mean_score'] for r in results]
    evaluator_calls = [r['final_metrics']['total_evaluator_calls'] for r in results]
    total_costs = [r['llm_usage']['cost_analysis']['total_cost_usd'] for r in results]
    total_tokens = [r['llm_usage']['total_tokens'] for r in results]
    success_rates = [r['stability_metrics'].get('success_rate', 0) for r in results]
    
    # æå–baselineæ€§èƒ½
    baseline_scores = [r['baseline_performance']['summary']['best_score'] for r in results]
    baseline_stability = [r['baseline_performance']['stability_metrics']['stability_score'] for r in results]
    
    # è®¡ç®—æ”¹è¿›æ¯”ä¾‹
    improvements = [(best - baseline) / baseline * 100 
                   for best, baseline in zip(best_scores, baseline_scores) 
                   if baseline > 0]
    
    return {
        'performance_metrics': {
            'best_score': {
                'mean': np.mean(best_scores),
                'std': np.std(best_scores),
                'min': min(best_scores),
                'max': max(best_scores),
                'median': np.median(best_scores)
            },
            'mean_score': {
                'mean': np.mean(mean_scores),
                'std': np.std(mean_scores),
                'median': np.median(mean_scores)
            },
            'improvement_over_baseline': {
                'mean': np.mean(improvements) if improvements else 0,
                'std': np.std(improvements) if improvements else 0,
                'min': min(improvements) if improvements else 0,
                'max': max(improvements) if improvements else 0
            }
        },
        'cost_metrics': {
            'total_cost_usd': {
                'mean': np.mean(total_costs),
                'std': np.std(total_costs),
                'min': min(total_costs),
                'max': max(total_costs),
                'total': sum(total_costs)
            },
            'total_tokens': {
                'mean': np.mean(total_tokens),
                'std': np.std(total_tokens),
                'min': min(total_tokens),
                'max': max(total_tokens),
                'total': sum(total_tokens)
            }
        },
        'efficiency_metrics': {
            'evaluator_calls': {
                'mean': np.mean(evaluator_calls),
                'std': np.std(evaluator_calls),
                'total': sum(evaluator_calls)
            },
            'cost_efficiency': {
                'score_per_dollar': [score / cost for score, cost in zip(best_scores, total_costs) if cost > 0],
                'score_per_evaluation': [score / calls for score, calls in zip(best_scores, evaluator_calls) if calls > 0]
            }
        },
        'stability_metrics': {
            'success_rate': {
                'mean': np.mean(success_rates),
                'std': np.std(success_rates),
                'min': min(success_rates),
                'max': max(success_rates)
            },
            'baseline_comparison': {
                'baseline_score_mean': np.mean(baseline_scores),
                'baseline_stability_mean': np.mean(baseline_stability),
                'improvement_consistency': np.std(improvements) if improvements else float('inf')
            }
        }
    }

def print_experiment_summary(summary: dict):
    """æ‰“å°å®éªŒæ‘˜è¦"""
    print("\n" + "="*80)
    print("å®éªŒç»Ÿè®¡æ‘˜è¦")
    print("="*80)
    
    perf = summary['performance_metrics']
    cost = summary['cost_metrics']
    eff = summary['efficiency_metrics']
    stab = summary['stability_metrics']
    
    print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"  æœ€ä½³åˆ†æ•°: {perf['best_score']['mean']:.3f} Â± {perf['best_score']['std']:.3f}")
    print(f"  åˆ†æ•°èŒƒå›´: [{perf['best_score']['min']:.3f}, {perf['best_score']['max']:.3f}]")
    print(f"  ç›¸å¯¹baselineæ”¹è¿›: {perf['improvement_over_baseline']['mean']:.1f}% Â± {perf['improvement_over_baseline']['std']:.1f}%")
    
    print(f"\nğŸ’° æˆæœ¬æŒ‡æ ‡:")
    print(f"  å¹³å‡æ€»æˆæœ¬: ${cost['total_cost_usd']['mean']:.6f} Â± ${cost['total_cost_usd']['std']:.6f}")
    print(f"  å¹³å‡Tokenä½¿ç”¨: {cost['total_tokens']['mean']:.0f} Â± {cost['total_tokens']['std']:.0f}")
    print(f"  æ€»æˆæœ¬: ${cost['total_cost_usd']['total']:.6f}")
    
    print(f"\nâš¡ æ•ˆç‡æŒ‡æ ‡:")
    print(f"  å¹³å‡è¯„ä¼°æ¬¡æ•°: {eff['evaluator_calls']['mean']:.0f} Â± {eff['evaluator_calls']['std']:.0f}")
    if eff['cost_efficiency']['score_per_dollar']:
        print(f"  æˆæœ¬æ•ˆç‡ (åˆ†æ•°/$): {np.mean(eff['cost_efficiency']['score_per_dollar']):.0f}")
    if eff['cost_efficiency']['score_per_evaluation']:
        print(f"  è¯„ä¼°æ•ˆç‡ (åˆ†æ•°/è¯„ä¼°): {np.mean(eff['cost_efficiency']['score_per_evaluation']):.3f}")
    
    print(f"\nğŸ¯ ç¨³å®šæ€§æŒ‡æ ‡:")
    print(f"  æˆåŠŸç‡: {stab['success_rate']['mean']:.3f} Â± {stab['success_rate']['std']:.3f}")
    print(f"  Baselineåˆ†æ•°: {stab['baseline_comparison']['baseline_score_mean']:.3f}")
    print(f"  æ”¹è¿›ä¸€è‡´æ€§: {stab['baseline_comparison']['improvement_consistency']:.3f}")

def run_comparative_experiments():
    """è¿è¡Œå¯¹æ¯”å®éªŒ"""
    
    # åŸºç¡€LLMé…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–
    api_key = os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        raise ValueError("DEEPSEEK_API_KEY not found in environment variables. Please create a .env file with your API key.")
    
    base_llm_config = {
        'host': os.getenv('DEFAULT_LLM_HOST', 'api.deepseek.com'),
        'key': api_key,
        'model': os.getenv('DEFAULT_LLM_MODEL', 'deepseek-chat'),
        'kwargs': {'timeout': int(os.getenv('DEFAULT_LLM_TIMEOUT', '300'))}
    }
    
    # å®éªŒé…ç½® - åŒ…æ‹¬FunSearchå’ŒHillClimbå¯¹æ¯”
    experiments = [
        # FunSearchå®éªŒé…ç½®
        {
            'name': 'funsearch_standard',
            'method_type': 'funsearch',
            'llm_config': base_llm_config,
            'method_config': {
                'max_sample_nums': 20,
                'num_samplers': 1,
                'num_evaluators': 1,
                'samples_per_prompt': 4,
                'debug_mode': False
            }
        },
        # HillClimbå®éªŒé…ç½®
        {
            'name': 'hillclimb_standard',
            'method_type': 'hillclimb',
            'llm_config': base_llm_config,
            'method_config': {
                'max_sample_nums': 20,
                'num_samplers': 1,
                'num_evaluators': 1,
                'debug_mode': False
            }
        }
    ]
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    all_experiment_results = {}
    
    for exp_config in experiments:
        results = run_comprehensive_experiment(
            experiment_name=exp_config['name'],
            llm_config=exp_config['llm_config'],
            method_config=exp_config['method_config'],
            method_type=exp_config['method_type'],
            num_runs=1
        )
        all_experiment_results[exp_config['name']] = results
    
    # ç”Ÿæˆè·¨å®éªŒå¯¹æ¯”æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("è·¨å®éªŒå¯¹æ¯”åˆ†æ (FunSearch vs HillClimb)")
    print(f"{'='*80}")
    
    comparison_table = []
    for exp_name, results in all_experiment_results.items():
        summary = results['experiment_summary']
        comparison_table.append({
            'experiment': exp_name,
            'method_type': results['method_type'],
            'avg_best_score': summary['performance_metrics']['best_score']['mean'],
            'score_std': summary['performance_metrics']['best_score']['std'],
            'avg_cost': summary['cost_metrics']['total_cost_usd']['mean'],
            'avg_tokens': summary['cost_metrics']['total_tokens']['mean'],
            'avg_evaluations': summary['efficiency_metrics']['evaluator_calls']['mean'],
            'success_rate': summary['stability_metrics']['success_rate']['mean'],
            'improvement': summary['performance_metrics']['improvement_over_baseline']['mean']
        })
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print(f"\n{'å®éªŒåç§°':<20} {'æ–¹æ³•':<10} {'å¹³å‡æœ€ä½³åˆ†æ•°':<12} {'åˆ†æ•°æ ‡å‡†å·®':<10} {'å¹³å‡æˆæœ¬($)':<12} {'å¹³å‡Token':<10} {'è¯„ä¼°æ¬¡æ•°':<8} {'æˆåŠŸç‡':<8} {'æ”¹è¿›(%)':<8}")
    print("-" * 120)
    for row in comparison_table:
        print(f"{row['experiment']:<20} {row['method_type']:<10} {row['avg_best_score']:<12.3f} {row['score_std']:<10.3f} {row['avg_cost']:<12.6f} {row['avg_tokens']:<10.0f} {row['avg_evaluations']:<8.0f} {row['success_rate']:<8.3f} {row['improvement']:<8.1f}")
    
    # æŒ‰æ–¹æ³•ç±»å‹åˆ†ç»„åˆ†æ
    funsearch_results = [row for row in comparison_table if row['method_type'] == 'funsearch']
    hillclimb_results = [row for row in comparison_table if row['method_type'] == 'hillclimb']
    
    print(f"\nğŸ“Š æ–¹æ³•å¯¹æ¯”åˆ†æ:")
    if funsearch_results:
        fs_avg_score = np.mean([row['avg_best_score'] for row in funsearch_results])
        fs_avg_cost = np.mean([row['avg_cost'] for row in funsearch_results])
        fs_avg_success = np.mean([row['success_rate'] for row in funsearch_results])
        print(f"  FunSearchå¹³å‡è¡¨ç°:")
        print(f"    å¹³å‡æœ€ä½³åˆ†æ•°: {fs_avg_score:.3f}")
        print(f"    å¹³å‡æˆæœ¬: ${fs_avg_cost:.6f}")
        print(f"    å¹³å‡æˆåŠŸç‡: {fs_avg_success:.3f}")
    
    if hillclimb_results:
        hc_avg_score = np.mean([row['avg_best_score'] for row in hillclimb_results])
        hc_avg_cost = np.mean([row['avg_cost'] for row in hillclimb_results])
        hc_avg_success = np.mean([row['success_rate'] for row in hillclimb_results])
        print(f"  HillClimbå¹³å‡è¡¨ç°:")
        print(f"    å¹³å‡æœ€ä½³åˆ†æ•°: {hc_avg_score:.3f}")
        print(f"    å¹³å‡æˆæœ¬: ${hc_avg_cost:.6f}")
        print(f"    å¹³å‡æˆåŠŸç‡: {hc_avg_success:.3f}")
    
    if funsearch_results and hillclimb_results:
        print(f"  æ–¹æ³•å¯¹æ¯”:")
        print(f"    åˆ†æ•°å·®å¼‚: FunSearch vs HillClimb = {fs_avg_score:.3f} vs {hc_avg_score:.3f} ({((fs_avg_score - hc_avg_score) / hc_avg_score * 100):+.1f}%)")
        print(f"    æˆæœ¬æ•ˆç‡: FunSearch={fs_avg_score/fs_avg_cost:.0f} vs HillClimb={hc_avg_score/hc_avg_cost:.0f} (åˆ†æ•°/ç¾å…ƒ)")
    
    # ä¿å­˜è·¨å®éªŒå¯¹æ¯”ç»“æœ
    cross_experiment_analysis = {
        'timestamp': time.time(),
        'experiments': list(all_experiment_results.keys()),
        'comparison_table': comparison_table,
        'method_comparison': {
            'funsearch_summary': {
                'avg_score': fs_avg_score if funsearch_results else None,
                'avg_cost': fs_avg_cost if funsearch_results else None,
                'avg_success_rate': fs_avg_success if funsearch_results else None,
                'configurations': [row['experiment'] for row in funsearch_results]
            },
            'hillclimb_summary': {
                'avg_score': hc_avg_score if hillclimb_results else None,
                'avg_cost': hc_avg_cost if hillclimb_results else None,
                'avg_success_rate': hc_avg_success if hillclimb_results else None,
                'configurations': [row['experiment'] for row in hillclimb_results]
            } if hillclimb_results else None
        },
        'detailed_results': all_experiment_results
    }
    
    os.makedirs('comprehensive_logs/cross_experiment_analysis', exist_ok=True)
    with open('comprehensive_logs/cross_experiment_analysis/funsearch_vs_hillclimb_comparison.json', 'w') as f:
        json.dump(cross_experiment_analysis, f, indent=2)
    
    # æ‰¾å‡ºæœ€ä¼˜é…ç½®
    best_by_score = max(comparison_table, key=lambda x: x['avg_best_score'])
    best_by_efficiency = min(comparison_table, key=lambda x: x['avg_cost'] / max(x['avg_best_score'], 1e-8))
    most_stable = max(comparison_table, key=lambda x: x['success_rate'])
    
    print(f"\nğŸ† æœ€ä¼˜é…ç½®åˆ†æ:")
    print(f"  æœ€é«˜åˆ†æ•°: {best_by_score['experiment']} ({best_by_score['method_type']}) - åˆ†æ•°: {best_by_score['avg_best_score']:.3f}")
    print(f"  æœ€é«˜æ•ˆç‡: {best_by_efficiency['experiment']} ({best_by_efficiency['method_type']}) - æˆæœ¬æ•ˆç‡: {best_by_efficiency['avg_cost']/max(best_by_efficiency['avg_best_score'], 1e-8):.6f}")
    print(f"  æœ€ç¨³å®š: {most_stable['experiment']} ({most_stable['method_type']}) - æˆåŠŸç‡: {most_stable['success_rate']:.3f}")
    
    return cross_experiment_analysis

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ç»¼åˆå¤šç»´åº¦è¯„ä»·æŒ‡æ ‡å®éªŒ...")
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs('comprehensive_logs', exist_ok=True)
    
    # è¿è¡Œå¯¹æ¯”å®éªŒ
    cross_experiment_results = run_comparative_experiments()
    
    print(f"\nâœ… æ‰€æœ‰å®éªŒå®Œæˆ!")
    print(f"ğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: comprehensive_logs/")
    print(f"ğŸ“Š è·¨å®éªŒå¯¹æ¯”æŠ¥å‘Š: comprehensive_logs/cross_experiment_analysis/funsearch_vs_hillclimb_comparison.json")

if __name__ == '__main__':
    main() 