{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sampler and SamplerTrimmer\n",
    "\n",
    " > This tutorial gives details about two import classes to process LLM generated codes (known as \"samples\"). The `Sampler` class defines how to access to the LLM, while the `SamplerTrimmer` class trims the unuseful part of the code using abstract systax tree (ast) package.\n",
    "\n",
    "## Sampler class\n",
    "\n",
    "The `Sampler` class defines how to access to the LLM. The user can either deploy an LLM locally on your own device/server, or use LLM API. The user should create a new child class of the `Sampler` class (extend `Sampler`) and implement (override) the `draw_sample` function.\n",
    "\n",
    "### Initialization of the user-defined sampler class\n",
    "\n",
    "There is a keyword argument `auto_trim`  in the `Sampler` class of which the default value is `True`. This means no matter the user chooses a code completion model (such as StarCoder, CodeLlama-Python, etc.) or a chat model (GPT series, Llama series, etc.), we can automatically identify the “useful part” without descriptions and truncated code. So, if there is no special issue, please **always leave it default**.\n",
    "\n",
    "### Implementation of the draw_sample function\n",
    "\n",
    "The `draw_sample` function decides the manner to obtain the generated content from LLM and return the `str` -typed content **(feel free to return the answer generated by LLM, which may incorporate some useless descriptions, as they will be trimmed automatically by our trimmer)**. Here, we show a brief example of using LLM API.\n",
    "\n",
    "## SamplerTrimmer class\n",
    "\n",
    "The following examples demonstrate how `SamplerTrimmer` works."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49f553aec4175b0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tutorial",
   "id": "f655a95b00dd724c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T14:59:25.515240Z",
     "start_time": "2024-10-21T14:59:22.147483Z"
    }
   },
   "source": "from llm4ad.base import SamplerTrimmer",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is an example of response content of LLM. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f0aad59a099bbc"
  },
  {
   "cell_type": "code",
   "source": [
    "llm_response_content = '''\\\n",
    "OK, this is the generated code:\n",
    "\n",
    "def my_function(arr):\n",
    "    \"\"\"This is an example function.\"\"\"\n",
    "    max = np.max(arr)\n",
    "    min = np.min(arr)\n",
    "    result = max / min\n",
    "    return result\n",
    "    \n",
    "This function aims to calculate the ...\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T14:59:25.526078Z",
     "start_time": "2024-10-21T14:59:25.522424Z"
    }
   },
   "id": "c6af14f07c00a6f3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our pipline, we only want the informative part, i.e., the code for the heuristic. So we can trim the redundant part (\"OK, this is ...\", \"This function aims to ...\") of the generated content by using the *SamplerTrimmer.auto_trim*. The *auto_trim* function can automatically identify if a response content is come from an instruct model (i.e., GPT-3.5) or a completion model (i.e., StarCoder), and perform correspond operations to trim the code.\n",
    "\n",
    "The trimmed result of the response content consists of **function body** and **descriptions** after the function body (don't worry about the content after the function body, as they can be removed easily)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31cf34e6119e1c98"
  },
  {
   "cell_type": "code",
   "source": [
    "trimmed_response_content = SamplerTrimmer.auto_trim(llm_response_content)\n",
    "print(trimmed_response_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T14:59:25.620833Z",
     "start_time": "2024-10-21T14:59:25.616120Z"
    }
   },
   "id": "13d2df87f4082240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"\"\"This is an example function.\"\"\"\n",
      "    max = np.max(arr)\n",
      "    min = np.min(arr)\n",
      "    result = max / min\n",
      "    return result\n",
      "    \n",
      "This function aims to calculate the ...\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert the trimmed response content (in str) to a Program instance by giving a template program."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4fa71944d91c852"
  },
  {
   "cell_type": "code",
   "source": [
    "template_program = '''\\\n",
    "import numpy as np\n",
    "\n",
    "def func(arr):\n",
    "    return arr\n",
    "'''\n",
    "\n",
    "program = SamplerTrimmer.sample_to_program(trimmed_response_content, template_program)\n",
    "print(str(program))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T14:59:25.642303Z",
     "start_time": "2024-10-21T14:59:25.637040Z"
    }
   },
   "id": "153fdd140340b4d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "\n",
      "def func(arr):\n",
      "    max = np.max(arr)\n",
      "    min = np.min(arr)\n",
      "    result = max / min\n",
      "    return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T14:59:25.680939Z",
     "start_time": "2024-10-21T14:59:25.677920Z"
    }
   },
   "id": "4d8edca85aa55d61",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
